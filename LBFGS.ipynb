{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"LBFGS.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_CwTVmOn1_mo","colab_type":"code","colab":{}},"source":["# Importing Libraries\n","from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import argparse, copy\n","\n","from model import *\n","from utils import *\n","from measures import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYMC0b8A2jHm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"d364ed79-c4d7-468f-846d-b30c2d753497","executionInfo":{"status":"ok","timestamp":1588866835507,"user_tz":-120,"elapsed":2679,"user":{"displayName":"Manjot Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjD4piq1M5zr5yEuoyniXrOb5V0DZB3SCvSlT03=s64","userId":"05742709412917757541"}}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device = tf.test.gpu_device_name()\n","if device != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device))\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","%cd /content/drive/My Drive/Colab Notebooks\n","# pip install -e ."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jsH40YuY1_ms","colab_type":"code","outputId":"1aa5814c-66a6-423e-9f3a-ff07aec4791f","executionInfo":{"status":"ok","timestamp":1588866841722,"user_tz":-120,"elapsed":3476,"user":{"displayName":"Manjot Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjD4piq1M5zr5yEuoyniXrOb5V0DZB3SCvSlT03=s64","userId":"05742709412917757541"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["batch_size = 64\n","test_batch_size = 1000\n","epochs = 14\n","lr = 1e-1\n","gamma = 0.7\n","seed = 1\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","use_cuda = torch.cuda.is_available()\n","train_loader, test_loader = dataloaders(batch_size, use_cuda)\n","print (len(train_loader))\n","\n","model_init = Net().to(device)\n","# CHANGE AS APPROPRIATE\n","# optimizer = optim.LBFGS(model.parameters(), lr=lr)\n","# scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","# saving checkpoints during/after training;;;can save other variables as well\n","# checkpoint = {'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}\n","train_losses, test_losses = [], []"],"execution_count":3,"outputs":[{"output_type":"stream","text":["938\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WLnNIKvWtN9C","colab_type":"code","colab":{}},"source":["# test accuracy problem.....\n","def main():\n","    train_size = 60032\n","    sigmas = [0.001,0.005,0.01,0.05,0.1]      #[0.001, 0.01, 0.1, 1.0] #, for 10 sharpness if very high\n","    delta = 1e-2\n","    mean = 0.0\n","    l_weight_norms = []\n","    l_sharpness = []\n","    # var_gradients = []  # maybe with respect to batch size.....\n","    model = copy.deepcopy(model_init)\n","    model.to(device)\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    #  optimizer = optim.Adagrad(model.parameters(), lr=lr)\n","    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","    for epoch in range(1, epochs + 1):\n","        train_loss = train(model, device, train_loader, optimizer, epoch, batch_size)\n","        train_losses.append(train_loss)\n","        test_loss = test(model, device, test_loader, batch_size)\n","        test_losses.append(test_loss)\n","        scheduler.step()\n","        # gradients = get_gradients(model)  # a list of values\n","        # grads.append(gradients) \n","    \"\"\"run for each sigma\"\"\"\n","    for sigma in sigmas:\n","        # grads = []\n","#         resetting model everytime\n","       \n","        # variance_score = compute_grad_variance(grads)\n","        # var_gradients.append(variance_score)\n","\n","        weight_l2_norm = cal_l2_norm(model)\n","        weight_bound = compute_bound(model, train_size, sigma, weight_l2_norm, delta)  #### last term \n","        l_weight_norms.append(weight_bound)\n","        print (\"norm bound:::\", weight_bound)\n","\n","#       to calculate sharpness, perturb the same model 10 times........then get expected value........\n","        num_times = 10\n","        p_train_losses = []\n","        for iterate in range(num_times):\n","            p_model = copy.deepcopy(model)\n","            # p_model.to(device)   # does not make any difference\n","            # p_optimizer = optim.SGD(p_model.parameters(), lr=lr)\n","            p_model = weight_pertubation(p_model, mean, sigma, device)\n","            p_train_loss = test(p_model, device, train_loader, batch_size)\n","            p_train_losses.append(p_train_loss)\n","        exp_p_error = sum(p_train_losses) / len(p_train_losses)\n","        # print (\"exp_p_error::\", exp_p_error)\n","        sharpness = exp_p_error - train_loss\n","        print (\"sharpness:::\", sharpness)\n","        l_sharpness.append(sharpness)\n","\n","        show_losses(train_losses, test_losses)  \n","    show_sharpness_norm(l_sharpness, l_weight_norms, sigmas)\n","\n","        \n","if __name__ == '__main__':\n","    main()\n","# Adam, Adagrad about 40% test accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQTnykw71_mv","colab_type":"code","colab":{}},"source":["# # test accuracy problem.....\n","# def main():\n","#     train_size = 60032\n","#     sigmas = [0.001,0.005,0.01,0.05,0.1]      #[0.001, 0.01, 0.1, 1.0] #, for 10 sharpness if very high\n","#     delta = 1e-2\n","#     mean = 0.0\n","#     l_weight_norms = []\n","#     l_sharpness = []\n","#     var_gradients = []  # maybe with respect to batch size.....\n","#     \"\"\"run for each sigma\"\"\"\n","#     for sigma in sigmas:\n","#         grads = []\n","# #         resetting model everytime\n","#         model = copy.deepcopy(model_init)\n","#         model.to(device)\n","#         # optimizer = optim.SGD(model.parameters(), lr=lr)\n","#         optimizer = optim.Adagrad(model.parameters(), lr=lr)\n","#         scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","#         for epoch in range(1, epochs + 1):\n","#             train_loss = train(model, device, train_loader, optimizer, epoch, batch_size)\n","#             train_losses.append(train_loss)\n","#             test_loss = test(model, device, test_loader, batch_size)\n","#             test_losses.append(test_loss)\n","#             scheduler.step()\n","#             gradients = get_gradients(model)  # a list of values\n","#             grads.append(gradients) \n","#         variance_score = compute_grad_variance(grads)\n","#         var_gradients.append(variance_score)\n","\n","#         weight_l2_norm = cal_l2_norm(model)\n","#         weight_bound = compute_bound(model, train_size, sigma, weight_l2_norm, delta)  #### last term \n","#         l_weight_norms.append(weight_bound)\n","#         print (\"norm bound:::\", weight_bound)\n","\n","# #       to calculate sharpness, perturb the same model 10 times........then get expected value........\n","#         num_times = 10\n","#         p_train_losses = []\n","#         for iterate in range(num_times):\n","#             p_model = copy.deepcopy(model)\n","#             # p_optimizer = optim.SGD(p_model.parameters(), lr=lr)\n","#             p_model = weight_pertubation(p_model, mean, sigma, device)\n","#             p_train_loss = test(p_model, device, train_loader, batch_size)\n","#             p_train_losses.append(p_train_loss)\n","#         exp_p_error = sum(p_train_losses) / len(p_train_losses)\n","#         print (\"exp_p_error::\", exp_p_error)\n","#         sharpness = exp_p_error - train_loss\n","#         l_sharpness.append(sharpness)\n","\n","#         show_losses(train_losses, test_losses)  \n","#     show_sharpness_norm(l_sharpness, l_weight_norms, sigmas)\n","\n","        \n","# if __name__ == '__main__':\n","#     main()\n","\n","# # we can vary many things............\n","# # RMSProp is working shot    \n","# # Adam, Adagrad about 40% test accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4GEs80nImDl","colab_type":"code","colab":{}},"source":["# norms = [297.2753, 29.7270, 2.9733, 0.3045]\n","# norms = torch.tensor(norms)\n","# norms = (norms - torch.mean(norms)) / norms.std()\n","# print (norms)\n","# exp_p_error = [0.316660638815209, 0.29393473547905175, 1.9219535219771013, 591.3643927659052]\n","# error = torch.tensor(exp_p_error)\n","# error = (error - torch.mean(error)) / error.std()\n","# print (error)\n","# # norm --- it is a tensor...\n","# sigmas = [0.001, 0.01, 0.1, 1.0] \n","# show_sharpness_norm(error, norms, sigmas)"],"execution_count":0,"outputs":[]}]}