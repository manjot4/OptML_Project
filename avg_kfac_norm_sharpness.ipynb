{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "avg_kfac_norm_sharpness.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-VB0IwQTdjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device = tf.test.gpu_device_name()\n",
        "if device != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "%cd /content/drive/My Drive/Colab Notebooks\n",
        "# RMS prop run again to get less std."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-eA2pU4Ukd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import argparse\n",
        "\n",
        "import copy\n",
        "from model import *\n",
        "from utils import *\n",
        "from kfac import *\n",
        "from kfac_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9bI9ANtUkg4",
        "colab_type": "code",
        "outputId": "e712a1a8-12fc-4adf-ead6-52105fff9c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses = []\n",
        "seeds = [12345, 1234, 123]\n",
        "for seed in seeds:\n",
        "    print (\"seed:::\", seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model Hyperparamters\n",
        "    batch_size = 64\n",
        "    epochs = 14\n",
        "    lrs = [3e-2, 3e-3, 3e-4, 3e-5, 3e-6]  \n",
        "    gamma = 0.7\n",
        "    momentum = 0.9\n",
        "    stat_decay = 0.95\n",
        "    damping = 1e-3\n",
        "    kl_clip = 1e-2\n",
        "    weight_decay = 3e-3\n",
        "    TCov = 10\n",
        "    TScal = 10\n",
        "    TInv = 100      \n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    train_loader, val_loader, test_loader, train_size, val_size = dataloaders(batch_size, use_cuda, seed)\n",
        "    print (train_size, val_size, len(test_loader.dataset))\n",
        "\n",
        "    model = Net().to(device)\n",
        "    \n",
        "    # Grid Search over different learning rates - over validation dataset\n",
        "    val_losses = []\n",
        "    for lr in lrs:\n",
        "        print (\"lr:\", lr)\n",
        "        model_copy = copy.deepcopy(model)\n",
        "        optimizer = KFACOptimizer(model_copy, lr=lr, momentum=momentum,stat_decay=stat_decay,damping=damping,kl_clip=kl_clip,\n",
        "                            weight_decay=weight_decay,TCov=TCov,TInv=TInv)\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_loss = train_kfac(model_copy, device, train_loader, optimizer, epoch, batch_size)\n",
        "            val_loss = test_2(model_copy, device, val_loader, batch_size, val_size)\n",
        "            # test_loss = test(model_copy, device, test_loader, batch_size)  # one can see on test_loss as well.....\n",
        "        val_losses.append(val_loss)\n",
        "    # print (\"Validation Losses::\\n\", val_losses)  \n",
        "    validation_losses.append(val_losses)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed::: 12345\n",
            "54000 6000 10000\n",
            "lr: 0.03\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.4486, Accuracy: 648/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "lr: 0.003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.3890, Accuracy: 657/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.2923, Accuracy: 662/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "lr: 0.0003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 0.1766, Accuracy: 5698/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.1369, Accuracy: 1085/6000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 635/6000 (11%)\n",
            "\n",
            "lr: 3e-05\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 1.4496, Accuracy: 4772/6000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1241, Accuracy: 5813/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0719, Accuracy: 5869/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0642, Accuracy: 5893/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0731, Accuracy: 5878/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0756, Accuracy: 5867/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0776, Accuracy: 5874/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0864, Accuracy: 5863/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0795, Accuracy: 5881/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0854, Accuracy: 5884/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0951, Accuracy: 5877/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1017, Accuracy: 5876/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1049, Accuracy: 5857/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1030, Accuracy: 5866/6000 (98%)\n",
            "\n",
            "lr: 3e-06\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2651, Accuracy: 1699/6000 (28%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5169, Accuracy: 4875/6000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5250, Accuracy: 5574/6000 (93%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3313, Accuracy: 5687/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2455, Accuracy: 5751/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1968, Accuracy: 5779/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1633, Accuracy: 5812/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1398, Accuracy: 5830/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1225, Accuracy: 5848/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1089, Accuracy: 5859/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0988, Accuracy: 5866/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0907, Accuracy: 5876/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0843, Accuracy: 5881/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0784, Accuracy: 5886/6000 (98%)\n",
            "\n",
            "seed::: 1234\n",
            "54000 6000 10000\n",
            "lr: 0.03\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.4630, Accuracy: 621/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "lr: 0.003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.4390, Accuracy: 612/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "lr: 0.0003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 0.1889, Accuracy: 5683/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.1157, Accuracy: 1262/6000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 605/6000 (10%)\n",
            "\n",
            "lr: 3e-05\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 1.3429, Accuracy: 5293/6000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1249, Accuracy: 5810/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0775, Accuracy: 5855/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0700, Accuracy: 5856/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0700, Accuracy: 5875/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0658, Accuracy: 5875/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0728, Accuracy: 5872/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0697, Accuracy: 5878/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0659, Accuracy: 5874/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0883, Accuracy: 5860/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0846, Accuracy: 5865/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0849, Accuracy: 5861/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0867, Accuracy: 5872/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0933, Accuracy: 5862/6000 (98%)\n",
            "\n",
            "lr: 3e-06\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2475, Accuracy: 1775/6000 (30%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4497, Accuracy: 5287/6000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5528, Accuracy: 5552/6000 (93%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3488, Accuracy: 5640/6000 (94%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2590, Accuracy: 5704/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2047, Accuracy: 5746/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1692, Accuracy: 5780/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1448, Accuracy: 5807/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1264, Accuracy: 5822/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1121, Accuracy: 5841/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1011, Accuracy: 5857/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0924, Accuracy: 5870/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0851, Accuracy: 5879/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0792, Accuracy: 5878/6000 (98%)\n",
            "\n",
            "seed::: 123\n",
            "54000 6000 10000\n",
            "lr: 0.03\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.3323, Accuracy: 670/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3123, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 659/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "lr: 0.003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.3029, Accuracy: 664/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3889, Accuracy: 657/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "lr: 0.0003\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 0.1763, Accuracy: 5703/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.2744, Accuracy: 873/6000 (15%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 658/6000 (11%)\n",
            "\n",
            "lr: 3e-05\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 1.3250, Accuracy: 5285/6000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1196, Accuracy: 5813/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0729, Accuracy: 5855/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0742, Accuracy: 5860/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0697, Accuracy: 5861/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0750, Accuracy: 5875/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0689, Accuracy: 5867/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0904, Accuracy: 5840/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0795, Accuracy: 5877/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0822, Accuracy: 5877/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0870, Accuracy: 5875/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0930, Accuracy: 5873/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1012, Accuracy: 5871/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1179, Accuracy: 5865/6000 (98%)\n",
            "\n",
            "lr: 3e-06\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2490, Accuracy: 1951/6000 (33%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4725, Accuracy: 5290/6000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5496, Accuracy: 5595/6000 (93%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3399, Accuracy: 5701/6000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2495, Accuracy: 5745/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1967, Accuracy: 5778/6000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1622, Accuracy: 5796/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1379, Accuracy: 5814/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1210, Accuracy: 5832/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1083, Accuracy: 5844/6000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0980, Accuracy: 5852/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0900, Accuracy: 5860/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0836, Accuracy: 5867/6000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0777, Accuracy: 5875/6000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-xlxQbqUkkD",
        "colab_type": "code",
        "outputId": "b3d5768b-6c93-4b66-b88d-85a8627bbc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print (validation_losses)\n",
        "validation_losses = np.mean(np.array(validation_losses), 0)\n",
        "print (\"averaged_validation losses::\\n\", validation_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.3025848789417998, 2.3025848789417998, 2.3025848789417998, 0.10295038765415232, 0.07840909420492802], [2.3025848789417998, 2.3025848789417998, 2.3025848789417998, 0.09333450418520481, 0.07922376632848953], [2.3025848789417998, 2.3025848789417998, 2.3025848789417998, 0.1178667300995043, 0.0777067448547546]]\n",
            "averaged_validation losses::\n",
            " [2.30258488 2.30258488 2.30258488 0.10471721 0.07844654]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnC_Ory7Ukmq",
        "colab_type": "code",
        "outputId": "c60e4d83-5f33-47e5-ee9b-33ebded3b806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Learning Rates - ['3e-2', '3e-3', '3e-4', '3e-5', '3e-6'] plotted on log scale\n",
        "lr_rates = np.log(np.array(lrs))\n",
        "fig = plt.figure()\n",
        "plt.plot(lr_rates, validation_losses, color='blue', marker='o') \n",
        "plt.xlabel('Learning Rates')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title(\"Validation performance over different learning rates\")\n",
        "plt.grid(True, linestyle='-.')\n",
        "fig.show()   \n",
        "# plt.legend(['3e-2', '3e-3', '3e-4', '3e-5', '3e-6'], loc='upper right')\n",
        "\"\"\"best lr = 3e-6 in case of NGD\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best lr = 3e-6 in case of RMSprop'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol-psszCUkph",
        "colab_type": "code",
        "outputId": "b883e73a-1483-4f9c-acba-d54254f81701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "avg_train_losses, avg_test_losses, avg_difference_test_train = [], [], []\n",
        "avg_weight_norms_list, avg_sharpness_list, avg_std_list, avg_mean_list = [], [], [], []\n",
        "\n",
        "\n",
        "seeds = [12345, 1234, 123]\n",
        "for seed in seeds:\n",
        "    print (\"seed:::\", seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model Hyperparamters\n",
        "    batch_size = 64\n",
        "    epochs = 14\n",
        "    lr = 3e-6 # good one - lowest validation loss among others\n",
        "    gamma = 0.7\n",
        "    momentum = 0.9\n",
        "    stat_decay = 0.95\n",
        "    damping = 1e-3\n",
        "    kl_clip = 1e-2\n",
        "    weight_decay = 3e-3\n",
        "    TCov = 10\n",
        "    TScal = 10\n",
        "    TInv = 100      \n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    train_loader, val_loader, test_loader, train_size, val_size = dataloaders(batch_size, use_cuda, seed)\n",
        "    print (train_size, val_size, len(test_loader.dataset))\n",
        "\n",
        "    model = Net().to(device)\n",
        "\n",
        "    train_losses, test_losses = [], []\n",
        "    sigmas = [0.001,0.003,0.006,0.009,0.03,0.06,0.09]    \n",
        "    delta = 1e-2\n",
        "    mean = 0.0\n",
        "    weight_norms_list = []\n",
        "    sharpness_list = []\n",
        "\n",
        "    optimizer = KFACOptimizer(model,lr=lr, momentum=momentum,stat_decay=stat_decay,damping=damping,kl_clip=kl_clip,\n",
        "                            weight_decay=weight_decay,TCov=TCov,TInv=TInv)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_kfac(model, device, train_loader, optimizer, epoch, batch_size)\n",
        "        train_losses.append(train_loss)\n",
        "        test_loss = test(model, device, test_loader, batch_size)\n",
        "        test_losses.append(test_loss)\n",
        "        # scheduler.step()\n",
        "    \n",
        "    avg_train_losses.append(train_losses) \n",
        "    avg_test_losses.append(test_losses)\n",
        "\n",
        "    # print train and test losses and their difference\n",
        "    difference_test_train = np.array(test_losses)  - np.array(train_losses)\n",
        "    avg_difference_test_train.append(difference_test_train)\n",
        "\n",
        "    print (\"running for sigmas\")\n",
        "    \"\"\"run for each sigma\"\"\"\n",
        "    std_list = []\n",
        "    mean_list = []\n",
        "\n",
        "    for sigma in sigmas:\n",
        "        weight_l2_norm = cal_l2_norm(model)\n",
        "        weight_bound = compute_bound(model, train_size, sigma, weight_l2_norm, delta)  \n",
        "        weight_norms_list.append(weight_bound)\n",
        "        # print (\"norm bound:::\", weight_bound)\n",
        "\n",
        "        #to calculate sharpness, perturb the same model 10 times, then get expected value\n",
        "        num_times = 20\n",
        "        p_train_losses = []\n",
        "        for iterate in range(num_times):\n",
        "            p_model = copy.deepcopy(model)\n",
        "            p_model = weight_pertubation(p_model, mean, sigma, device)\n",
        "            p_train_loss = test_2(p_model, device, train_loader, batch_size, train_size)\n",
        "            p_train_losses.append(p_train_loss)\n",
        "        exp_p_error = sum(p_train_losses) / len(p_train_losses)\n",
        "        std_list.append(np.std(np.array(p_train_losses)))\n",
        "        mean_list.append(np.mean(np.array(p_train_losses)))\n",
        "\n",
        "        sharpness = exp_p_error - train_loss\n",
        "        sharpness_list.append(sharpness)\n",
        "    \n",
        "    avg_weight_norms_list.append(weight_norms_list)\n",
        "    avg_sharpness_list.append(sharpness_list)\n",
        "    avg_std_list.append(std_list)\n",
        "    avg_mean_list.append(mean_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed::: 12345\n",
            "54000 6000 10000\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2651, Accuracy: 2740/10000 (27%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5316, Accuracy: 8109/10000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5164, Accuracy: 9357/10000 (94%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3194, Accuracy: 9499/10000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2367, Accuracy: 9575/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1873, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1564, Accuracy: 9691/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1352, Accuracy: 9720/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1178, Accuracy: 9743/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1059, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0958, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0890, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0819, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0767, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "running for sigmas\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53557/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53569/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53560/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53568/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53566/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53571/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53564/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53564/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53559/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53554/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53559/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53565/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 53557/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 53562/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 53561/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 53562/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53561/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 53560/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 53570/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 53557/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53518/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 53530/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53518/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 53539/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53541/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53546/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53550/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53530/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 53537/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53534/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0529, Accuracy: 53523/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53530/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53535/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0527, Accuracy: 53548/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53532/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0529, Accuracy: 53545/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53538/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53535/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 53532/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0529, Accuracy: 53527/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0592, Accuracy: 53477/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0592, Accuracy: 53452/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 53465/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53456/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0603, Accuracy: 53448/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0588, Accuracy: 53468/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0585, Accuracy: 53435/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0595, Accuracy: 53481/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 53453/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53443/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0618, Accuracy: 53452/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0605, Accuracy: 53455/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 53452/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0600, Accuracy: 53449/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0588, Accuracy: 53472/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0603, Accuracy: 53430/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 53454/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53459/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0596, Accuracy: 53477/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0593, Accuracy: 53468/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0701, Accuracy: 53357/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0712, Accuracy: 53354/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0679, Accuracy: 53387/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 53332/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0714, Accuracy: 53317/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0723, Accuracy: 53355/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0745, Accuracy: 53231/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0709, Accuracy: 53338/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0716, Accuracy: 53294/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 53299/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0684, Accuracy: 53375/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0721, Accuracy: 53360/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0725, Accuracy: 53336/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0715, Accuracy: 53310/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0687, Accuracy: 53389/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0718, Accuracy: 53303/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0730, Accuracy: 53318/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0706, Accuracy: 53318/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0689, Accuracy: 53375/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0702, Accuracy: 53327/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.4905, Accuracy: 46430/54000 (86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6993, Accuracy: 41736/54000 (77%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.0878, Accuracy: 34754/54000 (64%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7473, Accuracy: 40772/54000 (76%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.4454, Accuracy: 47298/54000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7541, Accuracy: 40675/54000 (75%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5574, Accuracy: 44863/54000 (83%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7150, Accuracy: 40613/54000 (75%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6470, Accuracy: 43085/54000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7050, Accuracy: 41566/54000 (77%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6526, Accuracy: 42392/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5761, Accuracy: 44536/54000 (82%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6600, Accuracy: 43050/54000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.9253, Accuracy: 38359/54000 (71%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5289, Accuracy: 45530/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5319, Accuracy: 45157/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7020, Accuracy: 42098/54000 (78%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5664, Accuracy: 44795/54000 (83%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6379, Accuracy: 42813/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7902, Accuracy: 40717/54000 (75%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.2518, Accuracy: 8439/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.1957, Accuracy: 14242/54000 (26%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.5655, Accuracy: 14236/54000 (26%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.1682, Accuracy: 10456/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.9588, Accuracy: 10332/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.7767, Accuracy: 13290/54000 (25%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.2567, Accuracy: 12954/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.9888, Accuracy: 9736/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.3166, Accuracy: 11534/54000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.8005, Accuracy: 13724/54000 (25%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.6367, Accuracy: 14217/54000 (26%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.4370, Accuracy: 15374/54000 (28%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 8.7180, Accuracy: 10839/54000 (20%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.5881, Accuracy: 11359/54000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.5287, Accuracy: 11665/54000 (22%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.3946, Accuracy: 13557/54000 (25%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.1844, Accuracy: 11266/54000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.5134, Accuracy: 7699/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.1606, Accuracy: 8687/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 10.4604, Accuracy: 6798/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.4016, Accuracy: 8401/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 25.7964, Accuracy: 5291/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 20.3674, Accuracy: 5278/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.1631, Accuracy: 6722/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 13.0147, Accuracy: 7344/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.3838, Accuracy: 9617/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 23.1406, Accuracy: 5083/54000 (9%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 12.3714, Accuracy: 7220/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.6632, Accuracy: 6446/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.7991, Accuracy: 5998/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.6795, Accuracy: 9990/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.2040, Accuracy: 7955/54000 (15%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.3459, Accuracy: 4166/54000 (8%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 17.0851, Accuracy: 5536/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 12.8501, Accuracy: 7837/54000 (15%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 19.1466, Accuracy: 6271/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.4355, Accuracy: 7246/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 19.4302, Accuracy: 5467/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.1200, Accuracy: 7089/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 19.3726, Accuracy: 7142/54000 (13%)\n",
            "\n",
            "seed::: 1234\n",
            "54000 6000 10000\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2431, Accuracy: 3028/10000 (30%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4234, Accuracy: 8959/10000 (90%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5304, Accuracy: 9359/10000 (94%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3272, Accuracy: 9491/10000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2393, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1895, Accuracy: 9650/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1565, Accuracy: 9693/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1340, Accuracy: 9728/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1170, Accuracy: 9743/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1051, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0954, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0875, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0822, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0762, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "running for sigmas\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53554/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53551/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53557/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53559/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53567/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53563/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 53560/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53549/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53546/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53553/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 53560/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53559/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53549/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 53561/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 53554/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53555/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53550/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 53562/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0511, Accuracy: 53565/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53558/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53528/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53541/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53519/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0538, Accuracy: 53498/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 53524/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53525/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53517/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53527/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53533/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53537/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53527/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53526/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53542/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53545/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53523/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53526/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 53527/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 53545/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53537/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 53543/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0591, Accuracy: 53470/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53422/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 53455/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53454/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0600, Accuracy: 53441/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0621, Accuracy: 53400/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0612, Accuracy: 53423/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0607, Accuracy: 53465/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0592, Accuracy: 53449/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53460/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0602, Accuracy: 53477/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0610, Accuracy: 53420/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0605, Accuracy: 53471/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0606, Accuracy: 53465/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0599, Accuracy: 53463/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53460/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0616, Accuracy: 53432/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0606, Accuracy: 53463/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53464/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0593, Accuracy: 53453/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0729, Accuracy: 53319/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0711, Accuracy: 53362/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0743, Accuracy: 53309/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 53339/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0740, Accuracy: 53272/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0711, Accuracy: 53335/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0725, Accuracy: 53317/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 53329/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0736, Accuracy: 53295/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0725, Accuracy: 53381/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0709, Accuracy: 53371/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0682, Accuracy: 53380/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0711, Accuracy: 53355/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0707, Accuracy: 53366/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 53298/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0700, Accuracy: 53333/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0713, Accuracy: 53345/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0730, Accuracy: 53323/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0705, Accuracy: 53380/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0722, Accuracy: 53339/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5873, Accuracy: 43915/54000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6521, Accuracy: 42835/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6421, Accuracy: 43554/54000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7279, Accuracy: 40507/54000 (75%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5467, Accuracy: 45990/54000 (85%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8047, Accuracy: 38296/54000 (71%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6140, Accuracy: 44103/54000 (82%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.4376, Accuracy: 47117/54000 (87%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6823, Accuracy: 42448/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5117, Accuracy: 46465/54000 (86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6342, Accuracy: 42928/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8636, Accuracy: 40079/54000 (74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7310, Accuracy: 41477/54000 (77%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.4776, Accuracy: 46682/54000 (86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6238, Accuracy: 43125/54000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5029, Accuracy: 46502/54000 (86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 45259/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5663, Accuracy: 44842/54000 (83%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.9052, Accuracy: 40889/54000 (76%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8973, Accuracy: 37115/54000 (69%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.9377, Accuracy: 12878/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.4073, Accuracy: 10396/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 8.6722, Accuracy: 6750/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.7142, Accuracy: 9752/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.0979, Accuracy: 9539/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.3493, Accuracy: 6435/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.4775, Accuracy: 10775/54000 (20%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.3357, Accuracy: 10348/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.9400, Accuracy: 7736/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 3.7772, Accuracy: 17637/54000 (33%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.9515, Accuracy: 11510/54000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.6617, Accuracy: 12433/54000 (23%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.4635, Accuracy: 12796/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.5033, Accuracy: 12828/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.1497, Accuracy: 12382/54000 (23%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.5042, Accuracy: 9324/54000 (17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.9793, Accuracy: 8620/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.0765, Accuracy: 12232/54000 (23%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.9775, Accuracy: 13048/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.6058, Accuracy: 9529/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.0620, Accuracy: 8892/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 20.1417, Accuracy: 8369/54000 (15%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 20.1767, Accuracy: 5532/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 12.9307, Accuracy: 7712/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.0008, Accuracy: 7129/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.0192, Accuracy: 7228/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.6291, Accuracy: 6019/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.1976, Accuracy: 7479/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 19.2924, Accuracy: 7787/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.5944, Accuracy: 6131/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 21.4495, Accuracy: 5765/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.0306, Accuracy: 8598/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.4096, Accuracy: 8420/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.8393, Accuracy: 5343/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.5562, Accuracy: 5759/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.8934, Accuracy: 5930/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 17.4737, Accuracy: 10152/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 13.0904, Accuracy: 9420/54000 (17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 23.8391, Accuracy: 4862/54000 (9%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 21.3635, Accuracy: 6858/54000 (13%)\n",
            "\n",
            "seed::: 123\n",
            "54000 6000 10000\n",
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "=> We keep following layers in KFAC. \n",
            "(0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "(1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "(2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "\n",
            "Test set: Average loss: 2.2452, Accuracy: 3510/10000 (35%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4268, Accuracy: 8921/10000 (89%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5357, Accuracy: 9375/10000 (94%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3303, Accuracy: 9511/10000 (95%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.2422, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1906, Accuracy: 9640/10000 (96%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1583, Accuracy: 9686/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1347, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1188, Accuracy: 9749/10000 (97%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.1063, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0965, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0869, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0803, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0757, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "running for sigmas\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53575/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53572/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53568/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53573/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 53577/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53573/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53570/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0515, Accuracy: 53578/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53569/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53567/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53570/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53575/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53573/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53573/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53570/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53575/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53578/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53569/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 53570/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 53574/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53542/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53540/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53544/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53538/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 53538/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53542/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 53546/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0536, Accuracy: 53552/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 53546/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0536, Accuracy: 53529/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53542/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53536/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0538, Accuracy: 53535/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0536, Accuracy: 53541/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 53532/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53549/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 53536/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53532/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 53544/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 53520/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0591, Accuracy: 53482/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0629, Accuracy: 53424/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0602, Accuracy: 53443/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53471/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0606, Accuracy: 53455/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53461/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 53468/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0595, Accuracy: 53466/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0612, Accuracy: 53436/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53467/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 53454/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53491/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0610, Accuracy: 53451/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0596, Accuracy: 53488/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 53477/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 53466/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 53469/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0607, Accuracy: 53444/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0594, Accuracy: 53470/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0630, Accuracy: 53411/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0712, Accuracy: 53312/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0700, Accuracy: 53388/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0709, Accuracy: 53351/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0697, Accuracy: 53360/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0736, Accuracy: 53353/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 53306/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0731, Accuracy: 53347/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0731, Accuracy: 53330/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0693, Accuracy: 53353/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0686, Accuracy: 53339/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0742, Accuracy: 53308/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0715, Accuracy: 53342/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0714, Accuracy: 53301/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0720, Accuracy: 53272/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0722, Accuracy: 53296/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0706, Accuracy: 53332/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0712, Accuracy: 53356/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0717, Accuracy: 53378/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0717, Accuracy: 53308/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 53390/54000 (99%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6333, Accuracy: 42369/54000 (78%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8358, Accuracy: 40060/54000 (74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5666, Accuracy: 44906/54000 (83%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6139, Accuracy: 43332/54000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8457, Accuracy: 41189/54000 (76%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5978, Accuracy: 44538/54000 (82%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6299, Accuracy: 43216/54000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7155, Accuracy: 42621/54000 (79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7497, Accuracy: 41233/54000 (76%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6151, Accuracy: 43961/54000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8127, Accuracy: 39704/54000 (74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6910, Accuracy: 41491/54000 (77%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5414, Accuracy: 45487/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.4400, Accuracy: 47700/54000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5452, Accuracy: 45049/54000 (83%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7267, Accuracy: 40262/54000 (75%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6238, Accuracy: 43477/54000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.6452, Accuracy: 43654/54000 (81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5435, Accuracy: 45248/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.5097, Accuracy: 45626/54000 (84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.3715, Accuracy: 12082/54000 (22%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.7087, Accuracy: 10428/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.4390, Accuracy: 14168/54000 (26%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.9828, Accuracy: 11862/54000 (22%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.5772, Accuracy: 9174/54000 (17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.5125, Accuracy: 12470/54000 (23%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.5574, Accuracy: 11176/54000 (21%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 5.6331, Accuracy: 11008/54000 (20%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.5509, Accuracy: 13003/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.9833, Accuracy: 9531/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.9657, Accuracy: 7505/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.3995, Accuracy: 10232/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.8829, Accuracy: 12088/54000 (22%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.0848, Accuracy: 15001/54000 (28%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 6.7671, Accuracy: 13386/54000 (25%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.4931, Accuracy: 13048/54000 (24%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.5913, Accuracy: 6141/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 7.5413, Accuracy: 10046/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 8.7762, Accuracy: 9632/54000 (18%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 9.7359, Accuracy: 7489/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.2556, Accuracy: 9426/54000 (17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.7898, Accuracy: 4860/54000 (9%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.3295, Accuracy: 4916/54000 (9%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 14.7992, Accuracy: 5927/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.5496, Accuracy: 5844/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.9049, Accuracy: 6342/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 17.9381, Accuracy: 9374/54000 (17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 18.3226, Accuracy: 6884/54000 (13%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.8434, Accuracy: 5682/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.9327, Accuracy: 8195/54000 (15%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 16.8656, Accuracy: 8561/54000 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 24.2589, Accuracy: 4383/54000 (8%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 12.5860, Accuracy: 10338/54000 (19%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 23.6341, Accuracy: 6492/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.0365, Accuracy: 5269/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 17.9523, Accuracy: 6637/54000 (12%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 20.0385, Accuracy: 7456/54000 (14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 15.8547, Accuracy: 6100/54000 (11%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 23.6677, Accuracy: 5157/54000 (10%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 20.1651, Accuracy: 8364/54000 (15%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki4TRqxuUksk",
        "colab_type": "code",
        "outputId": "2ba83ba1-46e5-47a8-8243-1e4bf5fbd5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "print (\"avg_train_losses::\\n\", avg_train_losses)\n",
        "print (\"avg_test_losses::\\n\", avg_test_losses)\n",
        "print (\"avg_difference_test_train::\\n\", avg_difference_test_train)\n",
        "print (\"avg_weight_norms_list::\\n\", avg_weight_norms_list)\n",
        "print (\"avg_sharpness_list::\\n\", avg_sharpness_list)\n",
        "print (\"avg_std_list::\\n\", avg_std_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_train_losses::\n",
            " [[2.293616466330126, 2.0138070885604025, 0.8729676952788615, 0.41131979703761956, 0.2784168365724844, 0.20988390654749214, 0.1665847938754971, 0.13673145617071486, 0.11442732469783434, 0.09719065063460884, 0.08387153894503675, 0.07297755841836687, 0.0638740456605692, 0.05613360314969838], [2.2886222914496868, 1.9763635028312556, 0.8752517762695443, 0.4233797841939316, 0.2827155371218637, 0.2106445329837607, 0.16606372808463765, 0.13600428529101385, 0.1136486171502002, 0.09693681668507827, 0.08362934141644934, 0.07275979679907668, 0.06383289959528858, 0.056240220487965224], [2.288203093960387, 1.9857275323562713, 0.8883153182494132, 0.42658727845591954, 0.2862797016675156, 0.21400330198037115, 0.16969134599429453, 0.13886096543010096, 0.1162990156331616, 0.09914409073943634, 0.08549419562751648, 0.0738882009575576, 0.0643265473135522, 0.05649815416360777]]\n",
            "avg_test_losses::\n",
            " [[2.265145122625266, 1.5315834952008194, 0.516418752966413, 0.3194476107882846, 0.2367484258238677, 0.1873156549824271, 0.156446128703986, 0.13516262311274838, 0.11779148978697267, 0.10591663999162662, 0.09577936074061758, 0.08901752355941542, 0.08190782873588763, 0.07666054581570778], [2.2430580786079357, 1.4233519028706156, 0.5304086202648794, 0.32715250039176574, 0.23927316724494763, 0.18952520837070078, 0.15652121413665213, 0.13401166553709917, 0.11703949893830688, 0.10514423424366173, 0.09543731103941894, 0.0874827970649786, 0.08223480648200983, 0.07618074764491646], [2.2451775757370482, 1.4268378353422615, 0.5356644953891729, 0.3303388224285879, 0.24223120994628614, 0.19062099857315137, 0.15826626126743426, 0.1346573422479022, 0.11883975866778641, 0.10631208223806825, 0.09648135378007676, 0.08693560988754984, 0.0803036070221169, 0.07571242861212439]]\n",
            "avg_difference_test_train::\n",
            " [array([-0.02847134, -0.48222359, -0.35654894, -0.09187219, -0.04166841,\n",
            "       -0.02256825, -0.01013867, -0.00156883,  0.00336417,  0.00872599,\n",
            "        0.01190782,  0.01603997,  0.01803378,  0.02052694]), array([-0.04556421, -0.5530116 , -0.34484316, -0.09622728, -0.04344237,\n",
            "       -0.02111932, -0.00954251, -0.00199262,  0.00339088,  0.00820742,\n",
            "        0.01180797,  0.014723  ,  0.01840191,  0.01994053]), array([-0.04302552, -0.5588897 , -0.35265082, -0.09624846, -0.04404849,\n",
            "       -0.0233823 , -0.01142508, -0.00420362,  0.00254074,  0.00716799,\n",
            "        0.01098716,  0.01304741,  0.01597706,  0.01921427])]\n",
            "avg_weight_norms_list::\n",
            " [[333.7490539550781, 111.24970245361328, 55.62488555908203, 37.083290100097656, 11.125184059143066, 5.562915802001953, 3.7089695930480957], [333.897705078125, 111.29925537109375, 55.649658203125, 37.09980773925781, 11.130138397216797, 5.565392971038818, 3.7106211185455322], [333.88836669921875, 111.29613494873047, 55.64809799194336, 37.0987663269043, 11.129826545715332, 5.565237045288086, 3.710517168045044]]\n",
            "avg_sharpness_list::\n",
            " [[-0.005238895986441047, -0.0030661466967323373, 0.0036591574014495643, 0.014709994674029087, 0.6148703827990563, 6.138928391437943, 16.932403353714246], [-0.0051146005935772745, -0.0029053132928095818, 0.004016502022575495, 0.015483288618215164, 0.5915675932144243, 6.022864759053097, 17.29325334234825], [-0.005162360370423988, -0.002983847408504686, 0.0039061531388107987, 0.014982935257688589, 0.5876168719082364, 6.371208623758743, 18.029729206359534]]\n",
            "avg_std_list::\n",
            " [[5.095601850871481e-05, 0.0001735164269184699, 0.0007253307217571671, 0.001632856387578883, 0.14630834988316707, 1.3342546847481893, 3.4116187363669512], [5.2182011890727094e-05, 0.0001844985250873991, 0.0007540203618151214, 0.0015576653634838693, 0.13403562306394698, 1.2764158215792925, 2.973363317371402], [4.576634603030375e-05, 0.00014898133962403944, 0.0009956834833622823, 0.0015248528835053652, 0.10767830581922148, 1.3212679500773847, 3.013528688012374]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUtbOUFUkve",
        "colab_type": "code",
        "outputId": "88127be5-940b-45d8-9059-6589d5e7333f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Average over 3 random seeds\n",
        "avg_train_losses = np.mean(np.array(avg_train_losses), 0)\n",
        "avg_test_losses = np.mean(np.array(avg_test_losses), 0)\n",
        "avg_difference_test_train = np.mean(np.array(avg_difference_test_train), 0)\n",
        "\n",
        "avg_weight_norms_list = np.mean(np.array(avg_weight_norms_list), 0)\n",
        "avg_sharpness_list = np.mean(np.array(avg_sharpness_list), 0)\n",
        "avg_std_list = np.mean(np.array(avg_std_list), 0)\n",
        "avg_mean_list = np.mean(np.array(avg_mean_list), 0)\n",
        "\n",
        "\n",
        "print (\"avg_train_losses:::\\n\", avg_train_losses)\n",
        "print (\"avg_test_losses:::\\n\", avg_test_losses)\n",
        "print (\"average difference list::\\n\", avg_difference_test_train)\n",
        "print (\"avg_weight_norms_list::\\n\", avg_weight_norms_list)\n",
        "print (\"avg_sharpness_list::\\n\", avg_sharpness_list)\n",
        "print (\"avg_std_list::\\n\", avg_std_list)\n",
        "print (\"avg_mean_list::\\n\", avg_mean_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_train_losses:::\n",
            " [2.29014728 1.99196604 0.87884493 0.42042895 0.28247069 0.21151058\n",
            " 0.16744662 0.1371989  0.11479165 0.09775719 0.08433169 0.07320852\n",
            " 0.06401116 0.05629066]\n",
            "avg_test_losses:::\n",
            " [2.25112693 1.46059108 0.52749729 0.32564631 0.2394176  0.18915395\n",
            " 0.15707787 0.13461054 0.11789025 0.10579099 0.09589934 0.08781198\n",
            " 0.08148208 0.07618457]\n",
            "average difference list::\n",
            " [-0.03902036 -0.53137496 -0.35134764 -0.09478264 -0.04305309 -0.02235663\n",
            " -0.01036875 -0.00258836  0.0030986   0.0080338   0.01156765  0.01460346\n",
            "  0.01747092  0.01989391]\n",
            "avg_weight_norms_list::\n",
            " [333.84504191 111.28169759  55.64088058  37.09395472  11.128383\n",
            "   5.56451527   3.71003596]\n",
            "avg_sharpness_list::\n",
            " [-5.17195232e-03 -2.98510247e-03  3.86060419e-03  1.50587395e-02\n",
            "  5.98018283e-01  6.17766726e+00  1.74184620e+01]\n",
            "avg_std_list::\n",
            " [4.96347921e-05 1.68998764e-04 8.25011522e-04 1.57179154e-03\n",
            " 1.29340760e-01 1.31064615e+00 3.13283691e+00]\n",
            "avg_mean_list::\n",
            " [ 0.05111871  0.05330556  0.06015126  0.0713494   0.65430894  6.23395792\n",
            " 17.47475263]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1LC5GxjUkyY",
        "colab_type": "code",
        "outputId": "0fef0b74-93a0-4826-d1c7-dbf1321389f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "show_losses(avg_train_losses, avg_test_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP8FSMw7Uk2F",
        "colab_type": "code",
        "outputId": "9a7d61ef-c742-4692-aeb5-978e79b9698d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(np.log(avg_weight_norms_list), avg_sharpness_list, marker='o')\n",
        "plt.grid(True, linestyle='-.')\n",
        "plt.title(\"Weight Norm and Sharpness - NGD\")\n",
        "plt.xlabel(\"Weight Norm\")\n",
        "plt.ylabel(\"Sharpness\")\n",
        "plt.show()\n",
        "# plt.legend() #loc='lower left'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSmV8su1rJZV",
        "colab_type": "code",
        "outputId": "ee109dbc-3531-47cf-cb25-325f740f0fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print (train_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.1163159096128004, 0.3691678487814928, 0.06829075171431236, 0.026734713976982082, 0.015211458048690552, 0.01268925697275689, 0.011409533685691196, 0.016703215771977087, 0.011761619426945657, 0.014741226857096636, 0.01238554253980023, 0.011729503646316404, 0.011061688999852864, 0.007981451490057535, 0.008939192874629659, 0.015514482640864337, 0.010903348609239858, 0.011422302856405765, 0.004283629952236388, 0.00880858252239877, 0.006632027507569835, 0.009127929578292497]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RELQd5IEwfuI",
        "colab_type": "code",
        "outputId": "991acb62-0fbb-4cfa-dd0a-6b68876cafe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print (avg_train_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.12366819 0.37398934 0.06662568 0.02604144 0.0156139  0.01222103\n",
            " 0.01059693 0.0157978  0.01131742 0.01750394 0.01433845 0.01082505\n",
            " 0.01086576 0.00982891 0.00929895 0.0099873  0.00731231 0.0092401\n",
            " 0.00432785 0.00880843 0.00756168 0.00966355]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}