{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32060,
     "status": "ok",
     "timestamp": 1591952567176,
     "user": {
      "displayName": "Manjot Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjD4piq1M5zr5yEuoyniXrOb5V0DZB3SCvSlT03=s64",
      "userId": "05742709412917757541"
     },
     "user_tz": -120
    },
    "id": "ALy15We42OkT",
    "outputId": "d9981351-b8b5-48c9-b280-b884458c8a7f"
   },
   "outputs": [],
   "source": [
    "\"\"\" This notebook contains code for computing variance of gradients for RMSprop for 3 different learning rates.\n",
    "Results are averaged over 3 random seeds to minimize the variation\"\"\"\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device = tf.test.gpu_device_name()\n",
    "if device != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device))\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "%cd /content/drive/My Drive/Colab Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oj-ENcIs2X2t"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse, copy\n",
    "\n",
    "from model import *\n",
    "from utils import *\n",
    "from measures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPYqRZM_2X5S"
   },
   "outputs": [],
   "source": [
    "lrs = [3e-6, 9e-6, 3e-5]\n",
    "avg_train_losses_lrs, avg_test_losses_lrs, avg_difference_test_train_lrs, avg_var_grad_lrs  = [], [], [], []\n",
    "\n",
    "for lr in lrs:\n",
    "    print (\"learning rate::\", lr)\n",
    "\n",
    "    avg_train_losses, avg_test_losses, avg_difference_test_train, avg_var_grad = [], [], [], []\n",
    "\n",
    "    seeds = [12345, 1234, 123]\n",
    "    for seed in seeds:\n",
    "        print (\"seed:::\", seed)\n",
    "        torch.manual_seed(seed)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Model Hyperparamters\n",
    "        batch_size = 64\n",
    "        epochs = 14\n",
    "        lr = lr\n",
    "        gamma = 0.7\n",
    "        momentum = 0.9\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        train_loader, val_loader, test_loader, train_size, val_size = dataloaders(batch_size, use_cuda, seed)\n",
    "        print (train_size, val_size, len(test_loader.dataset))\n",
    "\n",
    "        model = Net().to(device)\n",
    "\n",
    "        train_losses, test_losses = [], []\n",
    "        var_grad_list = []\n",
    "\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, device, train_loader, optimizer, epoch, batch_size)\n",
    "            train_losses.append(train_loss)\n",
    "            train_loss, grad_norms, grad_avg = train_get_grad(model, device, train_loader, optimizer, epoch, batch_size)\n",
    "            test_loss = test(model, device, test_loader, batch_size)\n",
    "            test_losses.append(test_loss)\n",
    "            scheduler.step()\n",
    "\n",
    "            # computing var_score over one epoch\n",
    "            variance_score = compute_grad_variance(grad_norms, grad_avg)\n",
    "            var_grad_list.append(variance_score)\n",
    "        \n",
    "        avg_train_losses.append(train_losses) \n",
    "        avg_test_losses.append(test_losses)\n",
    "        avg_var_grad.append(var_grad_list)\n",
    "\n",
    "        # print train and test losses and their difference\n",
    "        difference_test_train = np.array(test_losses)  - np.array(train_losses)\n",
    "        avg_difference_test_train.append(difference_test_train)\n",
    "    \n",
    "    avg_train_losses_lrs.append(avg_train_losses)\n",
    "    avg_test_losses_lrs.append(avg_test_losses)\n",
    "    avg_difference_test_train_lrs.append(avg_difference_test_train)\n",
    "    avg_var_grad_lrs.append(avg_var_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwnHUcNnssrM"
   },
   "outputs": [],
   "source": [
    "# For each learning rate, averaging over 3 random seeds\n",
    "\n",
    "# for learning rate - 3e-6\n",
    "avg_train_losses_lrs[0] = np.mean(np.array(avg_train_losses_lrs[0]), 0)\n",
    "avg_test_losses_lrs[0] = np.mean(np.array(avg_test_losses_lrs[0]), 0)\n",
    "avg_difference_test_train_lrs[0] = np.mean(np.array(avg_difference_test_train_lrs[0]), 0)\n",
    "avg_var_grad_lrs[0] = np.mean(np.array(avg_var_grad_lrs[0]), 0)\n",
    "\n",
    "# for learning rate - 9e-6\n",
    "avg_train_losses_lrs[1] = np.mean(np.array(avg_train_losses_lrs[1]), 0)\n",
    "avg_test_losses_lrs[1] = np.mean(np.array(avg_test_losses_lrs[1]), 0)\n",
    "avg_difference_test_train_lrs[1] = np.mean(np.array(avg_difference_test_train_lrs[1]), 0)\n",
    "avg_var_grad_lrs[1] = np.mean(np.array(avg_var_grad_lrs[1]), 0)\n",
    "\n",
    "\n",
    "# for learning rate - 3e-5\n",
    "avg_train_losses_lrs[2] = np.mean(np.array(avg_train_losses_lrs[2]), 0)\n",
    "avg_test_losses_lrs[2] = np.mean(np.array(avg_test_losses_lrs[2]), 0)\n",
    "avg_difference_test_train_lrs[2] = np.mean(np.array(avg_difference_test_train_lrs[2]), 0)\n",
    "avg_var_grad_lrs[2] = np.mean(np.array(avg_var_grad_lrs[2]), 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeaeh7W_2X8K"
   },
   "outputs": [],
   "source": [
    "print (\"for learning rate - 3e-6::\\n\")\n",
    "print (\"avg_train_losses::\\n\", avg_train_losses_lrs[0])\n",
    "print (\"avg_test_losses::\\n\", avg_test_losses_lrs[0])\n",
    "print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[0])\n",
    "print (\"avg_var_grad::\\n\", avg_var_grad_lrs[0])\n",
    "\n",
    "print (\"for learning rate - 9e-6::\\n\")\n",
    "print (\"avg_train_losses::\\n\", avg_train_losses_lrs[1])\n",
    "print (\"avg_test_losses::\\n\", avg_test_losses_lrs[1])\n",
    "print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[1])\n",
    "print (\"avg_var_grad::\\n\", avg_var_grad_lrs[1])\n",
    "\n",
    "\n",
    "print (\"for learning rate - 3e-5::\\n\")\n",
    "print (\"avg_train_losses::\\n\", avg_train_losses_lrs[2])\n",
    "print (\"avg_test_losses::\\n\", avg_test_losses_lrs[2])\n",
    "print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[2])\n",
    "print (\"avg_var_grad::\\n\", avg_var_grad_lrs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iE9qLMHr2YB4"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import numpy as np\n",
    "plt.plot(np.arange(epochs), avg_var_grad_lrs[0], marker='o', color = \"g\", label = \"lr = 3e-6\")\n",
    "plt.plot(np.arange(epochs), avg_var_grad_lrs[1], marker='o', color = \"b\", label = \"lr = 9e-6\")\n",
    "plt.plot(np.arange(epochs), avg_var_grad_lrs[2], marker='o', color = \"r\", label = \"lr = 3e-5\")\n",
    "plt.grid(True, linestyle='-.')\n",
    "plt.title(\"Variance of Gradients - RMSPROP\")\n",
    "plt.ylabel(\"Variance of gradients\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\t  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9iOdcSRu0euufTU80SN26",
   "collapsed_sections": [],
   "name": "avg_rms_var_of_grad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
