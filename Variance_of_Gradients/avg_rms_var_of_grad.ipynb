{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"avg_rms_var_of_grad.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9iOdcSRu0euufTU80SN26"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ALy15We42OkT","colab_type":"code","outputId":"d9981351-b8b5-48c9-b280-b884458c8a7f","executionInfo":{"status":"ok","timestamp":1591952567176,"user_tz":-120,"elapsed":32060,"user":{"displayName":"Manjot Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjD4piq1M5zr5yEuoyniXrOb5V0DZB3SCvSlT03=s64","userId":"05742709412917757541"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["\"\"\" This notebook contains code for computing variance of gradients for RMSprop for 3 different learning rates.\n","Results are averaged over 3 random seeds to minimize the variation\"\"\"\n","\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","device = tf.test.gpu_device_name()\n","if device != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device))\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","%cd /content/drive/My Drive/Colab Notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oj-ENcIs2X2t","colab_type":"code","colab":{}},"source":["# Importing Libraries\n","from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import argparse, copy\n","\n","from model import *\n","from utils import *\n","from measures import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPYqRZM_2X5S","colab_type":"code","colab":{}},"source":["lrs = [3e-6, 9e-6, 3e-5]\n","avg_train_losses_lrs, avg_test_losses_lrs, avg_difference_test_train_lrs, avg_var_grad_lrs  = [], [], [], []\n","\n","for lr in lrs:\n","    print (\"learning rate::\", lr)\n","\n","    avg_train_losses, avg_test_losses, avg_difference_test_train, avg_var_grad = [], [], [], []\n","\n","    seeds = [12345, 1234, 123]\n","    for seed in seeds:\n","        print (\"seed:::\", seed)\n","        torch.manual_seed(seed)\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Model Hyperparamters\n","        batch_size = 64\n","        epochs = 14\n","        lr = lr\n","        gamma = 0.7\n","        momentum = 0.9\n","\n","        use_cuda = torch.cuda.is_available()\n","        train_loader, val_loader, test_loader, train_size, val_size = dataloaders(batch_size, use_cuda, seed)\n","        print (train_size, val_size, len(test_loader.dataset))\n","\n","        model = Net().to(device)\n","\n","        train_losses, test_losses = [], []\n","        var_grad_list = []\n","\n","        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n","        scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","\n","        for epoch in range(1, epochs + 1):\n","            train_loss = train(model, device, train_loader, optimizer, epoch, batch_size)\n","            train_losses.append(train_loss)\n","            train_loss, grad_norms, grad_avg = train_get_grad(model, device, train_loader, optimizer, epoch, batch_size)\n","            test_loss = test(model, device, test_loader, batch_size)\n","            test_losses.append(test_loss)\n","            scheduler.step()\n","\n","            # computing var_score over one epoch\n","            variance_score = compute_grad_variance(grad_norms, grad_avg)\n","            var_grad_list.append(variance_score)\n","        \n","        avg_train_losses.append(train_losses) \n","        avg_test_losses.append(test_losses)\n","        avg_var_grad.append(var_grad_list)\n","\n","        # print train and test losses and their difference\n","        difference_test_train = np.array(test_losses)  - np.array(train_losses)\n","        avg_difference_test_train.append(difference_test_train)\n","    \n","    avg_train_losses_lrs.append(avg_train_losses)\n","    avg_test_losses_lrs.append(avg_test_losses)\n","    avg_difference_test_train_lrs.append(avg_difference_test_train)\n","    avg_var_grad_lrs.append(avg_var_grad)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwnHUcNnssrM","colab_type":"code","colab":{}},"source":["# For each learning rate, averaging over 3 random seeds\n","\n","# for learning rate - 3e-6\n","avg_train_losses_lrs[0] = np.mean(np.array(avg_train_losses_lrs[0]), 0)\n","avg_test_losses_lrs[0] = np.mean(np.array(avg_test_losses_lrs[0]), 0)\n","avg_difference_test_train_lrs[0] = np.mean(np.array(avg_difference_test_train_lrs[0]), 0)\n","avg_var_grad_lrs[0] = np.mean(np.array(avg_var_grad_lrs[0]), 0)\n","\n","# for learning rate - 9e-6\n","avg_train_losses_lrs[1] = np.mean(np.array(avg_train_losses_lrs[1]), 0)\n","avg_test_losses_lrs[1] = np.mean(np.array(avg_test_losses_lrs[1]), 0)\n","avg_difference_test_train_lrs[1] = np.mean(np.array(avg_difference_test_train_lrs[1]), 0)\n","avg_var_grad_lrs[1] = np.mean(np.array(avg_var_grad_lrs[1]), 0)\n","\n","\n","# for learning rate - 3e-5\n","avg_train_losses_lrs[2] = np.mean(np.array(avg_train_losses_lrs[2]), 0)\n","avg_test_losses_lrs[2] = np.mean(np.array(avg_test_losses_lrs[2]), 0)\n","avg_difference_test_train_lrs[2] = np.mean(np.array(avg_difference_test_train_lrs[2]), 0)\n","avg_var_grad_lrs[2] = np.mean(np.array(avg_var_grad_lrs[2]), 0)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeaeh7W_2X8K","colab_type":"code","colab":{}},"source":["print (\"for learning rate - 3e-6::\\n\")\n","print (\"avg_train_losses::\\n\", avg_train_losses_lrs[0])\n","print (\"avg_test_losses::\\n\", avg_test_losses_lrs[0])\n","print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[0])\n","print (\"avg_var_grad::\\n\", avg_var_grad_lrs[0])\n","\n","print (\"for learning rate - 9e-6::\\n\")\n","print (\"avg_train_losses::\\n\", avg_train_losses_lrs[1])\n","print (\"avg_test_losses::\\n\", avg_test_losses_lrs[1])\n","print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[1])\n","print (\"avg_var_grad::\\n\", avg_var_grad_lrs[1])\n","\n","\n","print (\"for learning rate - 3e-5::\\n\")\n","print (\"avg_train_losses::\\n\", avg_train_losses_lrs[2])\n","print (\"avg_test_losses::\\n\", avg_test_losses_lrs[2])\n","print (\"avg_difference_test_train_lrs::\\n\", avg_difference_test_train_lrs[2])\n","print (\"avg_var_grad::\\n\", avg_var_grad_lrs[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iE9qLMHr2YB4","colab_type":"code","colab":{}},"source":["# Plotting\n","import numpy as np\n","plt.plot(np.arange(epochs), avg_var_grad_lrs[0], marker='o', color = \"g\", label = \"lr = 3e-6\")\n","plt.plot(np.arange(epochs), avg_var_grad_lrs[1], marker='o', color = \"b\", label = \"lr = 9e-6\")\n","plt.plot(np.arange(epochs), avg_var_grad_lrs[2], marker='o', color = \"r\", label = \"lr = 3e-5\")\n","plt.grid(True, linestyle='-.')\n","plt.title(\"Variance of Gradients - RMSPROP\")\n","plt.ylabel(\"Variance of gradients\")\n","plt.xlabel(\"Epochs\")\n","plt.legend(loc='upper right')\n","plt.show()\t  "],"execution_count":0,"outputs":[]}]}